{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Model Cleanup\n",
    "This notebook takes the finn-onnx FACILE model exported by the quant_train notebook and cleans it up. This notebook stops before converting to hls layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in FINN and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.infer_datatypes import InferDataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and tidy up brevitas export\n",
    "model = ModelWrapper(\"quant_models/facileV3_4b_1.onnx\")\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "#model.save(\"quant_models/facileV2_4b_500_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: global_in\n",
      "Output tensor name: global_out\n",
      "input shape: [1, 14]\n",
      "out shape: [1, 1]\n",
      "input dtype: DataType.UINT4\n",
      "out dtype: DataType.FLOAT32\n"
     ]
    }
   ],
   "source": [
    "#print input and output tensors and data types/shapes\n",
    "from finn.core.datatype import DataType\n",
    "\n",
    "in_tensor = model.graph.input[0].name\n",
    "out_tensor = model.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % in_tensor)\n",
    "print(\"Output tensor name: %s\" % out_tensor)\n",
    "in_shape = model.get_tensor_shape(in_tensor)\n",
    "out_shape = model.get_tensor_shape(out_tensor)\n",
    "print(\"input shape: \" + str(in_shape))\n",
    "print(\"out shape: \" + str(out_shape))\n",
    "model.set_tensor_datatype(in_tensor, DataType.UINT4)\n",
    "#model.set_tensor_datatype(out_tensor, DataType.FLOAT32)\n",
    "in_dtype = model.get_tensor_datatype(in_tensor)\n",
    "out_dtype = model.get_tensor_datatype(out_tensor)\n",
    "print(\"input dtype: \" + str(in_dtype))\n",
    "print(\"out dtype: \" + str(out_dtype))\n",
    "model.save(\"quant_models/facileV3_4b_1_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'quant_models/facileV3_4b_1_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbe8012eeb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize net in netron\n",
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"quant_models/facileV3_4b_1_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (229538, 14)\n",
      "X_val shape: (12752, 14)\n",
      "X_test shape: (12752, 14)\n",
      "Y_train shape: (229538, 1)\n",
      "Y_val shape: (12752, 1)\n",
      "Y_test shape: (12752, 1)\n",
      "Using saved split data\n",
      "MSE: [545.77648603]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_split_np_data\n",
    "import proc_for_infer as pfi\n",
    "from finn.core.onnx_exec import execute_onnx as exe_onnx\n",
    "\n",
    "datasets = load_split_np_data()\n",
    "\n",
    "batch_size=1\n",
    "inps = datasets[1]\n",
    "exp_out = datasets[4]\n",
    "valid_size = len(exp_out)\n",
    "num_batches = int(valid_size/batch_size)\n",
    "running_error_square = 0\n",
    "for i in range(0, num_batches):\n",
    "    #print(i)\n",
    "    batch = inps[(i*batch_size):((i+1)*batch_size)]\n",
    "    batch_exp_out = exp_out[(i*batch_size):((i+1)*batch_size)]\n",
    "    proc_batch = pfi.preproc(batch)\n",
    "    proc_batch = proc_batch.astype(\"float32\")\n",
    "    inp_dict = {in_tensor : proc_batch}\n",
    "    #batch_out = accel.execute(proc_batch)\n",
    "    out_dict = exe_onnx(model, inp_dict)\n",
    "    batch_out = out_dict[out_tensor]\n",
    "    batch_out = batch_out.astype(\"int8\")\n",
    "    #print(batch_out)\n",
    "    batch_proc_out = pfi.postproc(batch_out)\n",
    "    batch_errs = batch_proc_out-batch_exp_out\n",
    "    batch_sq_errs = batch_errs*batch_errs\n",
    "    running_error_square += sum(batch_sq_errs)\n",
    "print(\"MSE: \" + str(running_error_square / (num_batches * batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def execute_onnx(\n",
      "    model, input_dict, return_full_exec_context=False, start_node=None, end_node=None\n",
      "):\n",
      "    \"\"\"Executes given ONNX ModelWrapper with given named inputs.\n",
      "\n",
      "    If return_full_exec_context is False, a dict of named outputs is returned\n",
      "    as indicated by the model.graph.output.\n",
      "\n",
      "    If return return_full_exec_context is True, the full set of tensors used by\n",
      "    the execution (including inputs, weights, activations and final outputs)\n",
      "    will be returned as a dict.\n",
      "\n",
      "    When start_node and end_node are set to None, the whole graph is executed.\n",
      "    If they are set to particular ONNX nodes, only the subgraph between (and\n",
      "    including) those nodes is executed.\n",
      "    \"\"\"\n",
      "\n",
      "    if not model.check_all_tensor_shapes_specified():\n",
      "        raise Exception(\"Found unspecified tensor shapes, try infer_shapes\")\n",
      "    ret = model.analysis(ta.nodes_topologically_sorted)\n",
      "    assert (\n",
      "        ret[\"nodes_topologically_sorted\"] is True\n",
      "    ), \"\"\"Nodes must be\n",
      "    topologically sorted.\"\"\"\n",
      "\n",
      "    graph = model.graph\n",
      "    # first, we need to make sure that every variable required by the graph has\n",
      "    # some buffer associated with it. this includes graph inputs (which includes\n",
      "    # the input data as well as the trained parameters) and the graph ValueInfo\n",
      "    # (intermediate tensors between layers)\n",
      "    # this is provided by the execution_context, which is a dict of np.ndarray\n",
      "    execution_context = model.make_empty_exec_context()\n",
      "    # fill in any inputs provided to this function\n",
      "    for inp_name in input_dict.keys():\n",
      "        if inp_name in execution_context:\n",
      "            if execution_context[inp_name].shape == input_dict[inp_name].shape:\n",
      "                execution_context[inp_name] = input_dict[inp_name]\n",
      "            else:\n",
      "                raise Exception(\n",
      "                    \"Shape mismatch for provided input %s: found %s expected %s \"\n",
      "                    % (\n",
      "                        inp_name,\n",
      "                        str(execution_context[inp_name].shape),\n",
      "                        str(input_dict[inp_name].shape),\n",
      "                    )\n",
      "                )\n",
      "        # else:\n",
      "        # raise Exception(\"Provided input not found in graph context: %s\" % inp_name)\n",
      "\n",
      "    # check if model has an execution mode set\n",
      "    # if None, execute model node by node using execute_node()\n",
      "    # if set to \"remote_pynq\" execute model on PYNQ board\n",
      "    # if set to \"rtlsim\" execute model using pyverilator\n",
      "    model_exec_mode = model.get_metadata_prop(\"exec_mode\")\n",
      "    if (model_exec_mode is None) or (model_exec_mode == \"\"):\n",
      "        # execute the model node by node\n",
      "        # we can simply walk down the list since the ONNX spec guarantees that it is\n",
      "        # topologically sorted\n",
      "        subgraph = []\n",
      "        if start_node is None:\n",
      "            start_node = model.graph.node[0]\n",
      "        if end_node is None:\n",
      "            end_node = model.graph.node[-1]\n",
      "        # select the nodes between specified start/end nodes\n",
      "        start_ind = model.get_node_index(start_node)\n",
      "        end_ind = model.get_node_index(end_node) + 1\n",
      "        assert end_ind >= start_ind, \"Start/end nodes must define valid subgraph\"\n",
      "        subgraph = graph.node[start_ind:end_ind]\n",
      "        for node in subgraph:\n",
      "            if get_sanitize_quant_tensors() != 0:\n",
      "                # round input values to match quantization annotation\n",
      "                execution_context = sanitize_quant_values(\n",
      "                    model, node.input, execution_context\n",
      "                )\n",
      "            execute_node(node, execution_context, graph, return_full_exec_context)\n",
      "            if get_sanitize_quant_tensors() != 0:\n",
      "                # round output values to quantization annotation\n",
      "                execution_context = sanitize_quant_values(\n",
      "                    model, node.output, execution_context\n",
      "                )\n",
      "    elif model_exec_mode == \"remote_pynq\":\n",
      "        # use remote exec metadata built into model to execute on a remote PYNQ\n",
      "        remote_exec(model, execution_context)\n",
      "    elif model_exec_mode == \"rtlsim\":\n",
      "        # use stitched IP for rtlsim\n",
      "        rtlsim_exec(model, execution_context)\n",
      "    else:\n",
      "        raise Exception(\n",
      "            \"\"\"Metadata property \"exec_mode\" is set to an unknown value.\n",
      "        Can be left unset or has to be set to \"remote_pynq\" for remote execution\n",
      "        on PYNQ board or \"rtlsim\" for execution using pyverilator!\"\"\"\n",
      "        )\n",
      "\n",
      "    if return_full_exec_context:\n",
      "        return execution_context\n",
      "    else:\n",
      "        # provide outputs as dict\n",
      "        output_dict = dict()\n",
      "        for out_tensor in graph.output:\n",
      "            out_name = out_tensor.name\n",
      "            output_dict[out_name] = execution_context[out_name]\n",
      "        return output_dict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from finn.core.onnx_exec import execute_onnx as exe_onnx\n",
    "from finn.util.visualization import showSrc\n",
    "showSrc(exe_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ModelWrapper:\n",
      "    \"\"\"A wrapper around ONNX ModelProto that exposes some useful utility\n",
      "    functions for graph manipulation and exploration.\"\"\"\n",
      "\n",
      "    def __init__(self, onnx_model_proto, make_deepcopy=False):\n",
      "        \"\"\"Creates a ModelWrapper instance.\n",
      "        onnx_model_proto can be either a ModelProto instance, or a string\n",
      "        with the path to a stored .onnx file on disk, or serialized bytes.\n",
      "\n",
      "        make_deepcopy: controls whether a deep copy of the ModelProto\n",
      "        is made internally.\n",
      "        \"\"\"\n",
      "        if isinstance(onnx_model_proto, str):\n",
      "            assert os.path.isfile(onnx_model_proto)\n",
      "            self._model_proto = onnx.load(onnx_model_proto)\n",
      "        elif isinstance(onnx_model_proto, bytes):\n",
      "            self._model_proto = onnx.load_from_string(onnx_model_proto)\n",
      "        else:\n",
      "            if make_deepcopy:\n",
      "                self._model_proto = copy.deepcopy(onnx_model_proto)\n",
      "            else:\n",
      "                self._model_proto = onnx_model_proto\n",
      "        self.temporary_fix_oldstyle_domain()\n",
      "\n",
      "    def temporary_fix_oldstyle_domain(self):\n",
      "        found_oldstyle = False\n",
      "        for n in self.graph.node:\n",
      "            if n.domain == \"finn\":\n",
      "                n_backend = util.get_by_name(n.attribute, \"backend\")\n",
      "                if n_backend is not None:\n",
      "                    backend_value = n_backend.s.decode(\"UTF-8\")\n",
      "                    if backend_value == \"fpgadataflow\":\n",
      "                        n.domain = \"finn.custom_op.fpgadataflow\"\n",
      "                    else:\n",
      "                        warnings.warn(\"Can't fix domain for node \" + str(n))\n",
      "                else:\n",
      "                    n.domain = \"finn.custom_op.general\"\n",
      "                found_oldstyle = True\n",
      "        if found_oldstyle:\n",
      "            warnings.warn(\n",
      "                \"\"\"Some old-style domain attributes were automatically converted to new-style,\n",
      "                i.e. domain=finn to domain=finn.custom_op.<general|fpgadataflow|...>\"\"\"\n",
      "            )\n",
      "\n",
      "    @property\n",
      "    def graph(self):\n",
      "        \"\"\"Returns the graph of the model.\"\"\"\n",
      "        return self._model_proto.graph\n",
      "\n",
      "    @graph.setter\n",
      "    def graph(self, value):\n",
      "        \"\"\"Sets the graph of the model according to value\"\"\"\n",
      "        self._model_proto.graph = value\n",
      "\n",
      "    @property\n",
      "    def model(self):\n",
      "        \"\"\"Returns the model.\"\"\"\n",
      "        return self._model_proto\n",
      "\n",
      "    @model.setter\n",
      "    def model(self, value):\n",
      "        \"\"\"Sets the model according to value.\"\"\"\n",
      "        self._model_proto = value\n",
      "\n",
      "    def save(self, filename):\n",
      "        \"\"\"Saves the wrapper ONNX ModelProto into a file with given name.\"\"\"\n",
      "        onnx.save(self._model_proto, filename)\n",
      "\n",
      "    def analysis(self, analysis_fxn):\n",
      "        \"\"\"Runs given anaylsis_fxn on this model and return resulting dict.\"\"\"\n",
      "        return analysis_fxn(self)\n",
      "\n",
      "    def transform(\n",
      "        self, transformation, make_deepcopy=True, cleanup=True, fix_float64=True\n",
      "    ):\n",
      "        \"\"\"Applies given Transformation repeatedly until no more changes can be made\n",
      "        and returns a transformed ModelWrapper instance.\n",
      "\n",
      "        - make_deepcopy : operates on a new (deep)copy of model.\n",
      "        - fix_float64 : DoubleToSingleFloat correction before starting\n",
      "        - cleanup : execute cleanup transformations before returning\n",
      "        \"\"\"\n",
      "        transformed_model = self\n",
      "        if make_deepcopy:\n",
      "            transformed_model = copy.deepcopy(self)\n",
      "        if fix_float64:\n",
      "            (transformed_model, model_was_changed) = DoubleToSingleFloat().apply(\n",
      "                transformed_model\n",
      "            )\n",
      "        model_was_changed = True\n",
      "        while model_was_changed:\n",
      "            (transformed_model, model_was_changed) = transformation.apply(\n",
      "                transformed_model\n",
      "            )\n",
      "        if cleanup:\n",
      "            transformed_model.cleanup()\n",
      "        return transformed_model\n",
      "\n",
      "    def cleanup(self):\n",
      "        \"Run cleanup transformations on the model.\"\n",
      "        transformed_model = self\n",
      "        cleanup_transforms = [\n",
      "            RemoveUnusedTensors(),\n",
      "            RemoveStaticGraphInputs(),\n",
      "            SortGraph(),\n",
      "        ]\n",
      "        for trn in cleanup_transforms:\n",
      "            transformed_model = transformed_model.transform(\n",
      "                trn, cleanup=False, make_deepcopy=False\n",
      "            )\n",
      "        return transformed_model\n",
      "\n",
      "    def check_compatibility(self):\n",
      "        \"\"\"Checks this model for FINN compatibility:\n",
      "\n",
      "        * no embedded subgraphs\n",
      "\n",
      "        * all tensor shapes are specified, including activations\n",
      "\n",
      "        * all constants are initializers\n",
      "        \"\"\"\n",
      "        # TODO check for no embedded subgraphs\n",
      "        # TODO check that all shapes are inferred\n",
      "        # TODO check that all constants are initializers\n",
      "        return True\n",
      "\n",
      "    def get_tensor_datatype(self, tensor_name):\n",
      "        \"\"\"Returns the FINN DataType of tensor with given name.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        qnt_annotations = graph.quantization_annotation\n",
      "        ret = util.get_by_name(qnt_annotations, tensor_name, \"tensor_name\")\n",
      "        if ret is not None:\n",
      "            ret = util.get_by_name(\n",
      "                ret.quant_parameter_tensor_names, \"finn_datatype\", \"key\"\n",
      "            )\n",
      "            if ret is not None:\n",
      "                return DataType[ret.value]\n",
      "        # TODO maybe use native ONNX tensor type instead of assuming fp32?\n",
      "        return DataType[\"FLOAT32\"]\n",
      "\n",
      "    def set_tensor_datatype(self, tensor_name, datatype):\n",
      "        \"\"\"Sets the FINN DataType of tensor with given name.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        qnt_annotations = graph.quantization_annotation\n",
      "        ret = util.get_by_name(qnt_annotations, tensor_name, \"tensor_name\")\n",
      "        if ret is not None:\n",
      "            ret_dt = util.get_by_name(\n",
      "                ret.quant_parameter_tensor_names, \"finn_datatype\", \"key\"\n",
      "            )\n",
      "            if ret_dt is not None:\n",
      "                ret_dt.value = datatype.name\n",
      "            else:\n",
      "                dt = onnx.StringStringEntryProto()\n",
      "                dt.key = \"finn_datatype\"\n",
      "                dt.value = datatype.name\n",
      "                ret.quant_parameter_tensor_names.append(dt)\n",
      "        else:\n",
      "            qa = onnx.TensorAnnotation()\n",
      "            dt = onnx.StringStringEntryProto()\n",
      "            dt.key = \"finn_datatype\"\n",
      "            dt.value = datatype.name\n",
      "            qa.tensor_name = tensor_name\n",
      "            qa.quant_parameter_tensor_names.append(dt)\n",
      "            qnt_annotations.append(qa)\n",
      "\n",
      "    def get_tensor_valueinfo(self, tensor_name):\n",
      "        \"\"\"Returns ValueInfoProto of tensor with given name, if it has one.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        vi_names = [(x.name, x) for x in graph.input]\n",
      "        vi_names += [(x.name, x) for x in graph.output]\n",
      "        vi_names += [(x.name, x) for x in graph.value_info]\n",
      "        try:\n",
      "            vi_ind = [x[0] for x in vi_names].index(tensor_name)\n",
      "            vi = vi_names[vi_ind][1]\n",
      "            return vi\n",
      "        except ValueError:\n",
      "            return None\n",
      "\n",
      "    def get_tensor_shape(self, tensor_name):\n",
      "        \"\"\"Returns the shape of tensor with given name, if it has ValueInfoProto.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        vi_names = [(x.name, x) for x in graph.input]\n",
      "        vi_names += [(x.name, x) for x in graph.output]\n",
      "        vi_names += [(x.name, x) for x in graph.value_info]\n",
      "        try:\n",
      "            vi_ind = [x[0] for x in vi_names].index(tensor_name)\n",
      "            vi = vi_names[vi_ind][1]\n",
      "            dims = [x.dim_value for x in vi.type.tensor_type.shape.dim]\n",
      "            return dims\n",
      "        except ValueError:\n",
      "            return None\n",
      "\n",
      "    def set_tensor_shape(self, tensor_name, tensor_shape, dtype=TensorProto.FLOAT):\n",
      "        \"\"\"Assigns shape in ValueInfoProto for tensor with given name.\"\"\"\n",
      "        new_vi = oh.make_tensor_value_info(tensor_name, dtype, tensor_shape)\n",
      "        # find what container tis tensor's ValueInfo lives in\n",
      "        # if not found anywhere, we assume it's a new value_info\n",
      "        target_container = self.graph.value_info\n",
      "        if util.get_by_name(self.graph.input, tensor_name) is not None:\n",
      "            target_container = self.graph.input\n",
      "        if util.get_by_name(self.graph.output, tensor_name) is not None:\n",
      "            target_container = self.graph.output\n",
      "        # remove from target container and add new\n",
      "        util.remove_by_name(target_container, tensor_name)\n",
      "        target_container.append(new_vi)\n",
      "\n",
      "    def set_initializer(self, tensor_name, tensor_value):\n",
      "        \"\"\"Sets the initializer value for tensor with given name.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        # convert tensor_value (numpy array) into TensorProto w/ correct name\n",
      "        tensor_init_proto = np_helper.from_array(tensor_value)\n",
      "        tensor_init_proto.name = tensor_name\n",
      "        # first, remove if an initializer already exists\n",
      "        init_names = [x.name for x in graph.initializer]\n",
      "        try:\n",
      "            init_ind = init_names.index(tensor_name)\n",
      "            init_old = graph.initializer[init_ind]\n",
      "            graph.initializer.remove(init_old)\n",
      "        except ValueError:\n",
      "            pass\n",
      "        # create and insert new initializer\n",
      "        graph.initializer.append(tensor_init_proto)\n",
      "        # set shape\n",
      "        dtype = tensor_init_proto.data_type\n",
      "        self.set_tensor_shape(tensor_name, list(tensor_value.shape), dtype)\n",
      "\n",
      "    def rename_tensor(self, old_name, new_name):\n",
      "        \"\"\"Renames a tensor from old_name to new_name.\"\"\"\n",
      "        graph = self.graph\n",
      "        # sweep over inputs\n",
      "        if util.get_by_name(graph.input, old_name) is not None:\n",
      "            util.get_by_name(graph.input, old_name).name = new_name\n",
      "        # sweep over outputs\n",
      "        if util.get_by_name(graph.output, old_name) is not None:\n",
      "            util.get_by_name(graph.output, old_name).name = new_name\n",
      "        # sweep over value_info\n",
      "        if util.get_by_name(graph.value_info, old_name) is not None:\n",
      "            util.get_by_name(graph.value_info, old_name).name = new_name\n",
      "        # sweep over initializers\n",
      "        if util.get_by_name(graph.initializer, old_name) is not None:\n",
      "            util.get_by_name(graph.initializer, old_name).name = new_name\n",
      "        # sweep over quantization annotations\n",
      "        if (\n",
      "            util.get_by_name(graph.quantization_annotation, old_name, \"tensor_name\")\n",
      "            is not None\n",
      "        ):\n",
      "            util.get_by_name(\n",
      "                graph.quantization_annotation, old_name, \"tensor_name\"\n",
      "            ).tensor_name = new_name\n",
      "        # sweep over node i/o\n",
      "        for n in graph.node:\n",
      "            if old_name in n.input:\n",
      "                n.input[list(n.input).index(old_name)] = new_name\n",
      "            if old_name in n.output:\n",
      "                n.output[list(n.output).index(old_name)] = new_name\n",
      "\n",
      "    def get_initializer(self, tensor_name):\n",
      "        \"\"\"Gets the initializer value for tensor with given name, if any.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        init_names = [x.name for x in graph.initializer]\n",
      "        try:\n",
      "            init_ind = init_names.index(tensor_name)\n",
      "            return np_helper.to_array(graph.initializer[init_ind])\n",
      "        except ValueError:\n",
      "            return None\n",
      "\n",
      "    def find_producer(self, tensor_name):\n",
      "        \"\"\"Finds and returns the node that produces the tensor with given name.\"\"\"\n",
      "        for x in self._model_proto.graph.node:\n",
      "            if tensor_name in x.output:\n",
      "                return x\n",
      "        return None\n",
      "\n",
      "    def find_upstream(self, tensor_name, finder_fxn):\n",
      "        \"\"\"Follow the producer chain upstream, calling finder_fxn on each upstream\n",
      "        node until it returns True or there are no nodes left. Returns the list\n",
      "        of nodes visited, or None if finder_fxn did not return True.\"\"\"\n",
      "        visit_list = []\n",
      "        current_tensor = tensor_name\n",
      "        while True:\n",
      "            current_producer = self.find_producer(current_tensor)\n",
      "            if current_producer is None:\n",
      "                return []\n",
      "            else:\n",
      "                found = finder_fxn(current_producer)\n",
      "                visit_list.append(current_producer)\n",
      "                if found:\n",
      "                    return visit_list\n",
      "                else:\n",
      "                    current_tensor = current_producer.input[0]\n",
      "\n",
      "    def find_consumer(self, tensor_name):\n",
      "        \"\"\"Finds and returns the node that consumes the tensor with given name.\n",
      "        Currently only works for linear graphs.\"\"\"\n",
      "        all_inputs = [x.input[0] for x in self._model_proto.graph.node]\n",
      "        try:\n",
      "            consumer_ind = all_inputs.index(tensor_name)\n",
      "            return self._model_proto.graph.node[consumer_ind]\n",
      "        except ValueError:\n",
      "            return None\n",
      "\n",
      "    def find_consumers(self, tensor_name):\n",
      "        \"\"\"Finds and returns a list of the nodes that consume tensor with\n",
      "        given name.\"\"\"\n",
      "        consumers = []\n",
      "        for n in self._model_proto.graph.node:\n",
      "            for inp_tensor in n.input:\n",
      "                if inp_tensor == tensor_name:\n",
      "                    consumers.append(n)\n",
      "        if consumers != []:\n",
      "            return consumers\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def find_direct_successors(self, node):\n",
      "        \"\"\"Finds and returns a list of the nodes that are successors of\n",
      "        given node.\"\"\"\n",
      "        successors = []\n",
      "        for outp_tensor in node.output:\n",
      "            tensor_consumer_list = self.find_consumers(outp_tensor)\n",
      "            if tensor_consumer_list is not None:\n",
      "                for consumer in tensor_consumer_list:\n",
      "                    successors.append(consumer)\n",
      "        if successors != []:\n",
      "            return successors\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def find_direct_predecessors(self, node):\n",
      "        \"\"\"Finds and returns a list of the nodes that are predecessors of\n",
      "        given node.\"\"\"\n",
      "        predecessors = []\n",
      "        for inp_tensor in node.input:\n",
      "            producer = self.find_producer(inp_tensor)\n",
      "            if producer is not None:\n",
      "                predecessors.append(producer)\n",
      "        if predecessors != []:\n",
      "            return predecessors\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def is_fork_node(self, node):\n",
      "        \"\"\"Checks if the given node is a fork, that is, the node has multiple\n",
      "        direct successors\"\"\"\n",
      "        direct_successors = self.find_direct_successors(node)\n",
      "        is_fork = False if direct_successors is None else (len(direct_successors) > 1)\n",
      "        return is_fork\n",
      "\n",
      "    def is_join_node(self, node):\n",
      "        \"\"\"Checks if the given node is a join, that is, the node has multiple\n",
      "        direct predecessors\"\"\"\n",
      "        direct_predecessors = self.find_direct_predecessors(node)\n",
      "        is_join = (\n",
      "            False if direct_predecessors is None else (len(direct_predecessors) > 1)\n",
      "        )\n",
      "        return is_join\n",
      "\n",
      "    def get_all_tensor_names(self):\n",
      "        \"\"\"Returns a list of all (input, output and value_info) tensor names\n",
      "        in the graph.\"\"\"\n",
      "        graph = self.graph\n",
      "        names = [x.name for x in graph.value_info]\n",
      "        names += [x.name for x in graph.input]\n",
      "        names += [x.name for x in graph.output]\n",
      "        return names\n",
      "\n",
      "    def make_new_valueinfo_name(self):\n",
      "        \"\"\"Returns a name that can be used for a new value_info.\"\"\"\n",
      "        names = self.get_all_tensor_names()\n",
      "        candidate = util.random_string()\n",
      "        while candidate in names:\n",
      "            candidate = util.random_string()\n",
      "        return candidate\n",
      "\n",
      "    def make_empty_exec_context(self):\n",
      "        \"\"\"Creates an empty execution context for this model.\n",
      "\n",
      "        The execution context is a dictionary of all tensors used for the\n",
      "        inference computation. Any initializer values will be taken into\n",
      "        account, all other tensors will be zero.\"\"\"\n",
      "        execution_context = dict()\n",
      "        graph = self._model_proto.graph\n",
      "        # make empty tensors for all the graph inputs and outputs\n",
      "        for vi in graph.input:\n",
      "            new_tensor = onnxutil.valueinfo_to_tensor(vi)\n",
      "            execution_context[vi.name] = new_tensor\n",
      "        for vi in graph.output:\n",
      "            new_tensor = onnxutil.valueinfo_to_tensor(vi)\n",
      "            execution_context[vi.name] = new_tensor\n",
      "        # make empty tensors for all intermediate buffers\n",
      "        for vi in graph.value_info:\n",
      "            new_tensor = onnxutil.valueinfo_to_tensor(vi)\n",
      "            execution_context[vi.name] = new_tensor\n",
      "        # fill in the constants provided by the initializers (TensorProto to npy)\n",
      "        for t in graph.initializer:\n",
      "            execution_context[t.name] = np_helper.to_array(t)\n",
      "        return execution_context\n",
      "\n",
      "    def check_all_tensor_shapes_specified(self):\n",
      "        \"\"\"Checks whether all tensors have a specified shape (ValueInfo).\n",
      "        The ONNX standard allows for intermediate activations to have no\n",
      "        associated ValueInfo, but FINN expects this.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        ret = True\n",
      "        for n in graph.node:\n",
      "            for i in n.input:\n",
      "                ret = ret and (self.get_tensor_shape(i) is not None)\n",
      "            for o in n.output:\n",
      "                ret = ret and (self.get_tensor_shape(o) is not None)\n",
      "        return ret\n",
      "\n",
      "    def get_tensor_fanout(self, tensor_name):\n",
      "        \"\"\"Returns the number of nodes for which the tensor with given name is\n",
      "        as input.\"\"\"\n",
      "        graph = self.graph\n",
      "        fanout = 0\n",
      "        for n in graph.node:\n",
      "            if tensor_name in n.input:\n",
      "                fanout += 1\n",
      "        return fanout\n",
      "\n",
      "    def get_metadata_prop(self, key):\n",
      "        \"\"\"Returns the value associated with metadata_prop with given key,\n",
      "        or None otherwise.\"\"\"\n",
      "        metadata_prop = util.get_by_name(self.model.metadata_props, key, \"key\")\n",
      "        if metadata_prop is None:\n",
      "            return None\n",
      "        else:\n",
      "            return metadata_prop.value\n",
      "\n",
      "    def set_metadata_prop(self, key, value):\n",
      "        \"\"\"Sets metadata property with given key to the given value.\"\"\"\n",
      "        metadata_prop = util.get_by_name(self.model.metadata_props, key, \"key\")\n",
      "        if metadata_prop is None:\n",
      "            metadata_prop = onnx.StringStringEntryProto()\n",
      "            metadata_prop.key = key\n",
      "            metadata_prop.value = value\n",
      "            self.model.metadata_props.append(metadata_prop)\n",
      "        else:\n",
      "            metadata_prop.value = value\n",
      "\n",
      "    def get_nodes_by_op_type(self, op_type):\n",
      "        \"\"\"Returns a list of nodes with specified op_type.\"\"\"\n",
      "        return list(filter(lambda x: x.op_type == op_type, self.graph.node))\n",
      "\n",
      "    def get_finn_nodes(self):\n",
      "        \"\"\"Returns a list of nodes where domain == 'finn.*'.\"\"\"\n",
      "        return list(filter(lambda x: util.is_finn_op(x.domain), self.graph.node))\n",
      "\n",
      "    def get_non_finn_nodes(self):\n",
      "        \"\"\"Returns a list of nodes where domain != 'finn.*'.\"\"\"\n",
      "        return list(filter(lambda x: not util.is_finn_op(x.domain), self.graph.node))\n",
      "\n",
      "    def get_node_index(self, node):\n",
      "        \"\"\"Returns current index of given node.\"\"\"\n",
      "        n_ind = 0\n",
      "        try:\n",
      "            for n in self.graph.node:\n",
      "                if n == node:\n",
      "                    return n_ind\n",
      "                n_ind += 1\n",
      "        except ValueError:\n",
      "            return None\n",
      "\n",
      "    def get_tensor_layout(self, tensor_name):\n",
      "        \"\"\"Returns the data layout annotation of tensor with given name.\n",
      "        The data layout is expressed as a list of strings with as many\n",
      "        elements as the number of dimensions in the tensor shape. Each\n",
      "        string annotates what is contained in that dimension. If there is no\n",
      "        data layout annotation, None will be returned.\n",
      "        Examples of data layout annotations:\n",
      "        [\"N\", \"C\"] is tensor[batch][channel]\n",
      "        [\"N\", \"C\", \"H\", \"W\"] is tensor[batch][channel][height][width]\n",
      "        [\"N\", \"H\", \"W\", \"C\"] is tensor[batch][height][width][channel]\n",
      "        \"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        qnt_annotations = graph.quantization_annotation\n",
      "        ret = util.get_by_name(qnt_annotations, tensor_name, \"tensor_name\")\n",
      "        if ret is not None:\n",
      "            ret = util.get_by_name(\n",
      "                ret.quant_parameter_tensor_names, \"tensor_layout\", \"key\"\n",
      "            )\n",
      "            if ret is not None:\n",
      "                return eval(ret.value)\n",
      "        return None\n",
      "\n",
      "    def set_tensor_layout(self, tensor_name, data_layout):\n",
      "        \"\"\"Sets the data layout annotation of tensor with given name. See\n",
      "        get_tensor_layout for examples.\"\"\"\n",
      "        tensor_shape = self.get_tensor_shape(tensor_name)\n",
      "        assert type(data_layout) == list, \"data_layout must be a list\"\n",
      "        if tensor_shape is not None:\n",
      "            assert len(tensor_shape) == len(\n",
      "                data_layout\n",
      "            ), \"\"\"Mismatch between number\n",
      "            of dimensions of tensor shape and data layout annotation.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        qnt_annotations = graph.quantization_annotation\n",
      "        ret = util.get_by_name(qnt_annotations, tensor_name, \"tensor_name\")\n",
      "        if ret is not None:\n",
      "            ret_tl = util.get_by_name(\n",
      "                ret.quant_parameter_tensor_names, \"tensor_layout\", \"key\"\n",
      "            )\n",
      "            if ret_tl is not None:\n",
      "                ret_tl.value = str(data_layout)\n",
      "            else:\n",
      "                tl = onnx.StringStringEntryProto()\n",
      "                tl.key = \"tensor_layout\"\n",
      "                tl.value = str(data_layout)\n",
      "                ret.quant_parameter_tensor_names.append(tl)\n",
      "        else:\n",
      "            qa = onnx.TensorAnnotation()\n",
      "            dt = onnx.StringStringEntryProto()\n",
      "            dt.key = \"tensor_layout\"\n",
      "            dt.value = str(data_layout)\n",
      "            qa.tensor_name = tensor_name\n",
      "            qa.quant_parameter_tensor_names.append(dt)\n",
      "            qnt_annotations.append(qa)\n",
      "\n",
      "    def get_tensor_sparsity(self, tensor_name):\n",
      "        \"\"\"Returns the sparsity of a given tensor as dictionary.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        qnt_annotations = graph.quantization_annotation\n",
      "        ret = util.get_by_name(qnt_annotations, tensor_name, \"tensor_name\")\n",
      "        if ret is not None:\n",
      "            ret = util.get_by_name(\n",
      "                ret.quant_parameter_tensor_names, \"tensor_sparsity\", \"key\"\n",
      "            )\n",
      "            if ret is not None:\n",
      "                return eval(ret.value)\n",
      "        return None\n",
      "\n",
      "    def set_tensor_sparsity(self, tensor_name, sparsity_dict):\n",
      "        \"\"\"Sets the sparsity annotation of a tensor with given name.\"\"\"\n",
      "        graph = self._model_proto.graph\n",
      "        qnt_annotations = graph.quantization_annotation\n",
      "        ret = util.get_by_name(qnt_annotations, tensor_name, \"tensor_name\")\n",
      "        if ret is not None:\n",
      "            ret_ts = util.get_by_name(\n",
      "                ret.quant_parameter_tensor_names, \"tensor_sparsity\", \"key\"\n",
      "            )\n",
      "            if ret_ts is not None:\n",
      "                ret_ts.value = str(sparsity_dict)\n",
      "            else:\n",
      "                ts = onnx.StringStringEntryProto()\n",
      "                ts.key = \"tensor_sparsity\"\n",
      "                ts.value = str(sparsity_dict)\n",
      "                ret.quant_parameter_tensor_names.append(ts)\n",
      "        else:\n",
      "            qa = onnx.TensorAnnotation()\n",
      "            dt = onnx.StringStringEntryProto()\n",
      "            dt.key = \"tensor_sparsity\"\n",
      "            dt.value = str(sparsity_dict)\n",
      "            qa.tensor_name = tensor_name\n",
      "            qa.quant_parameter_tensor_names.append(dt)\n",
      "            qnt_annotations.append(qa)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showSrc(ModelWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
