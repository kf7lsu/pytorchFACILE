{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Model Cleanup\n",
    "This notebook takes the finn-onnx FACILE model exported by the quant_train notebook and cleans it up. This notebook stops before converting to hls layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in FINN and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.infer_datatypes import InferDataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and tidy up brevitas export\n",
    "model = ModelWrapper(\"quant_models/facileV3_6b_1.onnx\")\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "#model.save(\"quant_models/facileV2_4b_500_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: global_in\n",
      "Output tensor name: global_out\n",
      "input shape: [1, 14]\n",
      "out shape: [1, 1]\n",
      "input dtype: DataType.UINT6\n",
      "out dtype: DataType.FLOAT32\n"
     ]
    }
   ],
   "source": [
    "#print input and output tensors and data types/shapes\n",
    "from finn.core.datatype import DataType\n",
    "\n",
    "in_tensor = model.graph.input[0].name\n",
    "out_tensor = model.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % in_tensor)\n",
    "print(\"Output tensor name: %s\" % out_tensor)\n",
    "in_shape = model.get_tensor_shape(in_tensor)\n",
    "out_shape = model.get_tensor_shape(out_tensor)\n",
    "print(\"input shape: \" + str(in_shape))\n",
    "print(\"out shape: \" + str(out_shape))\n",
    "model.set_tensor_datatype(in_tensor, DataType.UINT6)\n",
    "#model.set_tensor_datatype(out_tensor, DataType.FLOAT32)\n",
    "in_dtype = model.get_tensor_datatype(in_tensor)\n",
    "out_dtype = model.get_tensor_datatype(out_tensor)\n",
    "print(\"input dtype: \" + str(in_dtype))\n",
    "print(\"out dtype: \" + str(out_dtype))\n",
    "model.save(\"quant_models/facileV3_6b_1_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'quant_models/facileV3_6b_1_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb589e661d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize net in netron\n",
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"quant_models/facileV3_6b_1_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (229538, 14)\n",
      "X_val shape: (12752, 14)\n",
      "X_test shape: (12752, 14)\n",
      "Y_train shape: (229538, 1)\n",
      "Y_val shape: (12752, 1)\n",
      "Y_test shape: (12752, 1)\n",
      "Using saved split data\n",
      "MSE: [147.02400386]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_split_np_data\n",
    "import proc_for_infer as pfi\n",
    "from finn.core.onnx_exec import execute_onnx as exe_onnx\n",
    "\n",
    "datasets = load_split_np_data()\n",
    "\n",
    "batch_size=1\n",
    "inps = datasets[1]\n",
    "exp_out = datasets[4]\n",
    "valid_size = len(exp_out)\n",
    "num_batches = int(valid_size/batch_size)\n",
    "running_error_square = 0\n",
    "exp_act_out = [[],[]]\n",
    "for i in range(0, num_batches):\n",
    "    #print(i)\n",
    "    batch = inps[(i*batch_size):((i+1)*batch_size)]\n",
    "    batch_exp_out = exp_out[(i*batch_size):((i+1)*batch_size)]\n",
    "    proc_batch = pfi.preproc(batch)\n",
    "    proc_batch = proc_batch.astype(\"float32\")\n",
    "    inp_dict = {in_tensor : proc_batch}\n",
    "    #batch_out = accel.execute(proc_batch)\n",
    "    out_dict = exe_onnx(model, inp_dict)\n",
    "    batch_out = out_dict[out_tensor]\n",
    "    batch_out = batch_out.astype(\"int8\")\n",
    "    #print(batch_out)\n",
    "    batch_proc_out = pfi.postproc(batch_out)\n",
    "    batch_errs = batch_proc_out-batch_exp_out\n",
    "    batch_sq_errs = batch_errs*batch_errs\n",
    "    running_error_square += sum(batch_sq_errs)\n",
    "    #print(batch_exp_out[0][0])\n",
    "    #print(batch_proc_out[0][0])\n",
    "    exp_act_out[0].append(batch_exp_out[0][0])\n",
    "    exp_act_out[1].append(batch_proc_out[0][0])\n",
    "print(\"MSE: \" + str(running_error_square / (num_batches * batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_loc = 'exp_act_comparisons/post_transform.pkl'\n",
    "with open(save_loc, 'wb') as file:\n",
    "    pickle.dump(exp_act_out, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
